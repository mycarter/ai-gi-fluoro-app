# --- Imports ---
import time
import re
import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- Load Credentials ---
load_dotenv()
RADIO_ID = os.getenv("RADIO_ID")
RADIO_PASSWORD = os.getenv("RADIO_PASSWORD")
print("DEBUG RADIO_ID:", RADIO_ID)
print("DEBUG RADIO_PASSWORD:", RADIO_PASSWORD)

# --- Constants ---
BASE_URL = "https://radiopaedia.org"
SEARCH_URL = "https://radiopaedia.org/search?modality=Fluoroscopy&page=1&scope=cases"
BASE_IMAGE_DIR = "fluoro_images"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# --- Setup ---
os.makedirs(BASE_IMAGE_DIR, exist_ok=True)

options = uc.ChromeOptions()
options.add_argument("--start-maximized")
driver = uc.Chrome(options=options)

def login_selenium(username, password):
    print("üîê Logging in via Selenium...")
    driver.get("https://radiopaedia.org/sessions/new?lang=us")
    try:
        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, "user_identity"))).send_keys(username)
        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, "user_password"))).send_keys(password)
        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.NAME, "commit"))).click()
        time.sleep(5)
        try:
            WebDriverWait(driver, 3).until(
                EC.element_to_be_clickable((By.CLASS_NAME, "js-terms-of-use-disclaimer-accept-button"))
            ).click()
        except: pass
        if "sessions/new" in driver.current_url:
            raise Exception("‚ùå Login returned to login page.")
        print("‚úÖ Login successful. Current page:", driver.current_url)
    except Exception as e:
        driver.save_screenshot("failed_login_debug2.png")
        print("‚ùå Login failed.")
        driver.quit()
        raise e

# --- Start ---
print("üöÄ Script started...")
login_selenium(RADIO_ID, RADIO_PASSWORD)

print(f"üîé Navigating to: {SEARCH_URL}")
driver.get(SEARCH_URL)
WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.search-result.search-result-case")))
print("‚úÖ Case results loaded.")

# --- Scrape Page ---
soup = BeautifulSoup(driver.page_source, "html.parser")
case_cards = soup.find_all("a", class_="search-result search-result-case")
print(f"üîç Found {len(case_cards)} cases on page 1.")

data = []
titles_seen = set()

for case in case_cards[:3]:  # Test only first 3
    sub_url = case.get("href")
    title_tag = case.find("h4", class_="search-result-title-text")
    title = title_tag.text.strip() if title_tag else "Untitled"
    if title in titles_seen:
        print(f"‚ö†Ô∏è Duplicate: Skipping already saved case '{title}'")
        continue
    titles_seen.add(title)
    print(f"üìÇ Extracting: {title}")

    full_url = BASE_URL + sub_url

    case_data = {
        'image': '', 'diagnosis': title, 'age': '', 'gender': '', 'presentation': '',
        'description': '', 'modality': '', 'discussion': '', 'tags': '',
        'systems': '', 'contributor': '', 'license': '', 'scroll_folder': '', 'url': full_url
    }

    try:
        driver.set_page_load_timeout(30)
        driver.get(full_url)
    except Exception as e:
        print(f"‚ùå Error loading case '{title}': {e}")
        continue

    sub_soup = BeautifulSoup(driver.page_source, "html.parser")

    def extract_field(label_text, alt_texts=[]):
        labels = [label_text] + alt_texts
        for div in sub_soup.find_all("div", class_="data-item"):
            label = div.find("strong", class_="data-item-label")
            if label:
                for l in labels:
                    if label.get_text(strip=True) == l:
                        sibling = label.find_next_sibling(string=True)
                        return sibling.strip() if sibling else ""
        return ""

    pres = sub_soup.find("div", id="case-patient-presentation")
    if pres and pres.p:
        case_data['presentation'] = pres.p.text.strip()

    case_data['gender'] = extract_field("Gender:")
    case_data['age'] = extract_field("Age:")
    case_data['modality'] = extract_field("Modality:", ["Imaging modality:"]) or "Fluoroscopy"
    case_data['discussion'] = extract_field("Discussion:", ["Case discussion:"])
    case_data['contributor'] = extract_field("Contributed by:", ["By:"])
    case_data['license'] = extract_field("Licence:", ["License:"])

    findings = sub_soup.find("div", class_="sub-section study-findings body")
    if findings:
        case_data['description'] = findings.get_text(" ", strip=True)

    tag_elements = sub_soup.find_all("a", class_="tag")
    case_data['tags'] = ", ".join(tag.text.strip() for tag in tag_elements)

    system_tags = sub_soup.find_all("span", class_="label label--system")
    case_data['systems'] = ", ".join(tag.text.strip() for tag in system_tags)

    related = sub_soup.find_all("a", class_="internal")
    related_articles = [a.text.strip() for a in related if '/articles/' in a['href']]
    if related_articles:
        case_data['tags'] += ", " + ", ".join(related_articles)

    system_folder = "unsorted"
    if case_data['systems']:
        system_folder = case_data['systems'].split(",")[0].strip().replace(" ", "_")
    image_folder = os.path.join(BASE_IMAGE_DIR, system_folder)
    os.makedirs(image_folder, exist_ok=True)
    case_data['scroll_folder'] = image_folder

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "img"))
        )
        time.sleep(1)

        image_elements = driver.find_elements(By.TAG_NAME, "img")
        image_urls = []

        for img in image_elements:
            src = img.get_attribute("src") or img.get_attribute("data-src")
            if src and ("case-images" in src or "/images/" in src):
                image_urls.append(src)

        if not image_urls:
            screenshot_path = os.path.join(image_folder, re.sub(r'[^a-zA-Z0-9_]', '_', title) + "_screenshot.png")
            driver.save_screenshot(screenshot_path)
            case_data['image'] = os.path.basename(screenshot_path)
            print(f"üñºÔ∏è Screenshot fallback saved for {title}")
        else:
            for i, img_url in enumerate(image_urls):
                img_name = re.sub(r'[^a-zA-Z0-9_]', '_', title) + f"_{i+1}.jpg"
                img_path = os.path.join(image_folder, img_name)

                img_resp = requests.get(img_url, headers=HEADERS, timeout=15)
                if img_resp.status_code == 200:
                    with open(img_path, "wb") as f:
                        f.write(img_resp.content)
                    if i == 0:
                        case_data['image'] = img_name
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to download image(s) for {title}: {e}")

    data.append(case_data)
    print(f"‚úÖ Saved: {title}")

# --- Save ---
df = pd.DataFrame(data)
df.to_csv("radiopaedia_cases.csv", index=False)
print("üéâ Done. Data saved to radiopaedia_cases.csv")
driver.quit()
